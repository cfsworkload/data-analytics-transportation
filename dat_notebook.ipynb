{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.clustering import KMeans\n",
    "from numpy import array\n",
    "from math import sqrt\n",
    "from pyspark.sql import SQLContext\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "import calendar\n",
    "import time\n",
    "import base64\n",
    "import requests\n",
    "import math\n",
    "\n",
    "#Globals for Kafka config\n",
    "OUTPUTTOPIC='newOutput'\n",
    "containerIP='0.0.0.0'\n",
    "\n",
    "URL = 'http://' + containerIP + ':9443/topics/' + OUTPUTTOPIC\n",
    "HEADERS = {'Content-Type': 'application/vnd.kafka.v1+json'}\n",
    "\n",
    "#last N entries in the database\n",
    "limitN = 20\n",
    "\n",
    "#Sensors to analyze\n",
    "sensors = ['PM41301', 'PM41801', 'PM107631']\n",
    "\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "def do_the_math(SensorID):\n",
    "    df1 = sqlContext.sql(\"SELECT codigo, intensidad, velocidad, tf, dt FROM madridtraffic WHERE codigo = \\'\" + SensorID + \"\\'\" + \" order by ts desc limit \" + str(limitN))\n",
    "    df1.show()\n",
    "    \n",
    "    if df1.rdd.isEmpty():\n",
    "        print \"There's no data set for the specified sensors.\"\n",
    "        return\n",
    "    \n",
    "    #mllib clustering function requires data in array and float valuesdf1\n",
    "    parsedData = df1.map(lambda row: array([float(row.intensidad), float(row.velocidad)]))\n",
    "    print('parsedData is mapped')\n",
    "    \n",
    "    #calculate mean for standardization\n",
    "    mean = parsedData.mean()\n",
    "    \n",
    "    #calculate standard deviation\n",
    "    std = parsedData.stdev()\n",
    "\n",
    "    processedData = parsedData.map(lambda x: ((x-mean)/std))\n",
    "    print 'normalized the data'\n",
    "\n",
    "    # Build the model (cluster the data)\n",
    "    clusters = KMeans.train(processedData, 2, maxIterations=10, runs=1, initializationMode=\"random\")\n",
    "    print 'clusters initilized'\n",
    "\n",
    "    #denormalize the centers.\n",
    "    centers = (clusters.centers*std)+mean\n",
    "    print 'denormalized the centers'\n",
    "\n",
    "\n",
    "    #midpoints-threshold\n",
    "    a = (centers[0][0]+centers[1][0])/2\n",
    "    b = (centers[0][1]+centers[1][1])/2\n",
    "\n",
    "    #creates dictionary\n",
    "    message = {\"codigo\": SensorID, \"newThresholds\":{\"ThresholdSpeed\":int(b), \"ThresholdIntensity\":int(a)}}\n",
    "    print message\n",
    "    message = base64.b64encode(str(message))\n",
    "\n",
    "    #creates dictionary\n",
    "    messages = {\"records\":[{\"value\":message}]}\n",
    "    #converts dictionary into json msg\n",
    "    Threshold = json.dumps(messages)\n",
    "    \n",
    "    #publish the message: payload is threshold and other field is topic\n",
    "    r = requests.post(URL, data=Threshold, headers=HEADERS)\n",
    "    print(r.text)\n",
    "    \n",
    "def kick_off_routine():\n",
    "    #Loop through the sensors\n",
    "    for i in sensors:\n",
    "        do_the_math(i)\n",
    "\n",
    "df = sqlContext.read.parquet(\"swift://DataServices.spark/DataServices/DataServices\")\n",
    "\n",
    "df = df.cache()\n",
    "df.registerTempTable(\"madridtraffic\")\n",
    "\n",
    "kick_off_routine()\n",
    "df.unpersist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}